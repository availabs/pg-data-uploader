#!/usr/bin/env node

'use strict'

/*
  data
    <source>
      shapefiles
        <geoType>
          <year>
            <state>
              <zip archive>
*/

const SOURCE_AGENCY = 'BTS'

const { promisify } = require('util')

const {
  exec,
  execSync,
} = require('child_process')

const {
  readdir,
  readdirSync,
  unlink,
} = require('fs')

const {
  join,
  extname,
} = require('path')

const extract = require('extract-zip')

const readdirAsync = promisify(readdir)
const unlinkAsync = promisify(unlink)
const extractAsync = promisify(extract)
const execAsync = promisify(exec)




const cliArgs = process.argv.slice(2)
const minimistOptions = {
	//treat all double hyphenated arguments without equal signs as boolean
	boolean: true,

	// an object mapping string names to strings or arrays of string argument names to use as aliases
	alias: { //
		agencies: 'agency',
		dataSourceFormat: 'dataSourceFormats',
		geographyType: 'geographyTypes',
		versions: 'version',
		states: 'state',
	}
}

let {
  pgConfigFilePath = join(__dirname, '../config/postgres_db.env'),
  dataDir = join(__dirname, '../data/'),
  pgfutterPath = join(__dirname, '../lib/pgfutter'),
	agencies: reqAgencies,
	dataSourceFormats: reqDataSourceFormats,
	geographyTypes: reqGeographyTypes,
  versions: reqVersions,
	states: reqStates,
	cleanup,
} = require('minimist')(cliArgs, minimistOptions);

const envFile = require('node-env-file')
envFile(pgConfigFilePath)

const pgEnv = {
  PGHOST     : process.env.NPMRDS_POSTGRES_NETLOC,
  PGPORT     : process.env.NPMRDS_POSTGRES_PORT || undefined,
  PGUSER     : process.env.NPMRDS_POSTGRES_USER,
  PGPASSWORD : process.env.NPMRDS_POSTGRES_PASSWORD || undefined,
  PGDATABASE : process.env.NPMRDS_POSTGRES_DB,
}

const pgConnectStr =
  `PG:host=${pgEnv.PGHOST} port=${pgEnv.PGPORT} user=${pgEnv.PGUSER} ` +
    `dbname=${pgEnv.PGDATABASE} password=${pgEnv.PGPASSWORD}`


// Convert the requested fields to arrays.
reqAgencies = reqAgencies && reqAgencies.split(',').map(s => s && s.trim()).filter(s => s)
reqDataSourceFormats = reqDataSourceFormats && reqDataSourceFormats.split(',').map(s => s && s.trim()).filter(s => s)
reqGeographyTypes = reqGeographyTypes && reqGeographyTypes.split(',').map(s => s && s.trim()).filter(s => s)
reqStates = reqStates && reqStates.split(',').map(s => s && s.trim()).filter(s => s)
reqVersions = reqVersions && reqVersions.split(',').map(s => s && s.trim()).filter(s => s)


async function loadData () {
  const agencies = reqAgencies || await readdirAsync(dataDir)

  for (let i = 0; i < agencies.length; ++i) {
    const agency = agencies[i]
    const agencyDir = join(dataDir, agency)

    const dataSourcesFormats = reqDataSourceFormats || await readdirAsync(agencyDir)

    for (let j = 0; j < dataSourceFormats.length; ++j) {
      const dataSourceFormat = dataSourceFormats[j]
      const dataSourceFormatDir = join(agencyDir, dataSourceFormat)

      const geographyTypes = reqGeographyTypes || await readdirAsync(dataSourceFormatDir)

      // Load the geographyTypes in parallel
      await Promise.all(
        geographyTypes.map(async (geographyType) => {
          const geographyTypeDir = join(dataSourceFormatDir, geoTypeDir)

          const versions = reqVersions || await readdirAsync(geographyTypeDir)

          for (let k = 0; k < vesions.length; ++k) {
            const version = versions[k]
            const versionDir = join(geographyTypeDir, version)

            const states = reqStates || aait readdirAsync(versionDir)

            for (let m = 0; m < states.length; ++m) {
              const state = states[m]
              const stateDir = join(versionDir, state)

            }
          }
        })
      )
    }
  }
}


async function loadShapefiles () {
  try {
    geographyTypes.forEach(async (geoType) => {

      const geoTypeDir = join(shapefilesDir, geoType)

      let versions
      try {
        versions = await readdirAsync(geoTypeDir)
      } catch (err) {
        if (err.code === 'ENOENT') {
          console.warn(`WARNING: no ${dataType} ${geoType} directory.`)
        } else {
          console.log(err.message)
        }
        return
      }

      versions.forEach(async (yr) => {
        const versionDir = join(geoTypeDir, yr)
        const states = await readdirAsync(versionDir)

        states.forEach(async (state) => {
          const stateDataDir = join(versionDir, state)
          const filenames = (await readdirAsync(stateDataDir))

          const zipFiles = filenames.filter(fname => extname(fname) === '.zip')

          if (zipFiles.length) {
            if (zipFiles.length > 1) {
              console.error(
                `INVARIANT BROKEN: more than one zip in shapefile subdirectory ${stateDataDir}`
              )
              return
            }

            await execAsync(`unzip -o ${join(stateDataDir, zipFiles[0])} -d ${stateDataDir}`)
          }

          const dataDir = stateDataDir
          const schema = state.toLowerCase()
          const tableNameBase = `${geoType}_shp`
          const tableName = `${tableNameBase}_v${yr}`

          try {
            const msg = `LOADED: "${schema}".${tableName}`
            console.time(msg)
            await shapeFileLoader({ dataDir, schema, tableNameBase, tableName })
            console.timeEnd(msg)
          } catch (err) {
            console.error(err)
          }

          await deleteShapefiles({ dataDir })
        })
      })
    })
  } catch (err) {
    console.error(err)
  }
}


async function shapeFileLoader (params) {

  const {
    dataDir,
    schema,
    tableNameBase,
    tableName,
  } = params

  /* Sample output of ogrinfo:

			INFO: Open of `.'
						using driver `ESRI Shapefile' successful.
			1: milbase (3D Polygon)
	*/
  // const ogrinfo = await execAsync(`ogrinfo ${dataDir}`)

  // const shapeFileInfo = ogrinfo.stdout.split('\n').filter(line => line.match(/\d+:/))
	
	// if (shapeFileInfo.length < 1) {
		// throw new Error(ogrinfo.stderr || `No ogrinfo for ${tableName}`)
	// } else if (shapeFileInfo.length > 1) {
		// throw new Error(`The shapefile loader currently does not support multiple shapes per shapefile: ${tableName}`)
	// }
	
  try {
    await execAsync(`psql -c 'CREATE SCHEMA IF NOT EXISTS "${schema}";'`, { env: pgEnv })
  } catch (err) {
		// Race-condition error: IF NOT EXISTS is not thread-safe. Annoying, but harmless.
		if (!err.message.match(/ERROR:  duplicate key value violates unique constraint/)) {
			throw err
		}
  }

  await execAsync(`psql -c 'DROP TABLE IF EXISTS "${schema}".${tableName};'`, { env: pgEnv })

  let cmd =
		`ogr2ogr -f PostgreSQL "${pgConnectStr}" ${dataDir} ` +
		`-lco SCHEMA=${schema} -lco OVERWRITE=YES -nln ${tableName}`

	try {
		try {  // First try without PROMOTE_TO_MULTI
			const output = await execAsync(cmd)
			if (output.stderr) {
				throw new Error(output.stderr)
			}
		} catch (err) { // If above failed, retry with PROMOTE_TO_MULTI
			const output = await execAsync(`${cmd} -nlt PROMOTE_TO_MULTI`)
			if (output.stderr) {
				throw new Error(output.stderr)
			}
		}
  	await handleTableInheritanceHierarchy(params)
	} catch (err) {
		throw err
	}
}


async function handleTableInheritanceHierarchy (params) {

  const {
    schema,
    tableNameBase,
    tableName,
  } = params

  // Create the state's parent table
  await execAsync(
    `psql -c 'CREATE TABLE IF NOT EXISTS "${schema}".${tableNameBase} (LIKE "${schema}".${tableName});'`,
    { env: pgEnv }
  )

  // Create the root table
  await execAsync(
    `psql -c 'CREATE TABLE IF NOT EXISTS public.${tableNameBase} (LIKE "${schema}".${tableNameBase});'`,
    { env: pgEnv }
  )

  // State's subset inherits from parent.
  await execAsync(
    `psql -c 'ALTER TABLE "${schema}".${tableName} INHERIT "${schema}".${tableNameBase};'`,
    { env: pgEnv }
  )

  // State's parent inherits from root.
  try {
    await execAsync(
      `psql -c 'ALTER TABLE "${schema}".${tableNameBase} INHERIT public.${tableNameBase};'`,
      { env: pgEnv }
    )
  } catch (err) {
    if (!err.message.match(/would be inherited from more than once/)) {
      console.error(err)
    }
  }
}


async function deleteShapefiles ({ dataDir }) {

  if (!cleanup) {
    return
  }

  const files = await readdirAsync(dataDir)
  const toDelete = files.filter(f => extname(f) !== '.zip')

  await Promise.all(toDelete.map(f => unlinkAsync(join(dataDir, f))))
}

Promise.all([
  loadShapefiles()
]).catch(console.error.bind(console))
